<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on 诗酒趁华年</title>
    <link>http://m.sjchn.com/tags/ml/</link>
    <description>Recent content in Ml on 诗酒趁华年</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sun, 18 Nov 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="http://m.sjchn.com/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>WOE 和 IV 介绍</title>
      <link>http://m.sjchn.com/p/2018/woe_n_iv/</link>
      <pubDate>Sun, 18 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>http://m.sjchn.com/p/2018/woe_n_iv/</guid>
      <description>在金融相关领域的建模中，WOE 和 IV 用的比较多。之前参加的一次征信相关比赛最后的评估指标就是特征的 IV 值，用于发现征信模型中的重要特征。
IV 值的计算是以 WOE 为基础的，因此先介绍 WOE，再引入 IV。
WOE WOE 的全称是 Weight Of Evidence. 要对一个特征计算 WOE，尤其是连续型的数值特征，需要先对特征做离散化处理，也叫做分组或分箱。
为方便叙述，假设共有 $M$ 个样本，其中正负样本数量分别为 $p_M$ 和 $n_M$ ，满足 $p_M + n_M = M$。只考虑单个特征，分组数量为 $N$，第 $i$ 个分组中正负样本数量分别为 $p_i$ 和 $n_i$。则对于第 $i$ 组，其 WOE 计算公式为：
$$ WOE_i = \ln{\frac{P_{p_i}}{P_{n_i}}} = \ln{\frac{p_i/p_M}{n_i/n_M}}$$
从公式中可以看出，WOE 表达的是当前分组中正样本占所有正样本的比例与当前分组中负样本占所有负样本的比例的比例关系。
由于可能出现某个分组中正负样本某个值为零的可能，导致公式取值为正无穷或负无穷，因此实际使用中，一般会加上一个常数 $\epsilon$, 范围是 $[0, 1]$, 一般的，$\epsilon = 0.5$。WOE 调整为：
$$ WOE_i = \ln{\frac{(p_i+ \epsilon)/p_M }{(n_i+\epsilon)/n_M}}$$
IV 有了 WOE 的基础，IV 的引入就很顺利了。从 WOE 的计算公式可以看出，WOE 有正有负，分别表达了特征在该分组上与整体样本是正相关还是负相关。可以稍加变化，直接表达特征在分组上与整体样本分布的相关程度，因此引入 IV 的计算公式：</description>
    </item>
    
    <item>
      <title>AUC 和 ROC</title>
      <link>http://m.sjchn.com/p/2016/auc-roc/</link>
      <pubDate>Mon, 28 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://m.sjchn.com/p/2016/auc-roc/</guid>
      <description>AUC 是点击率预估模型的常用评价指标，一般来说，AUC 越高，点击率模型越好，当然这也不是绝对的。虽然平时用 AUC 挺多的，但是一直没有深入去研究清楚，导致前几天被人问起的时候基本没答上来，很是忏愧。
AUC 其实是 Area Under Curve，然后这个 Curve 就是 ROC，全称是 Receiver Operating Characteristic，通常又叫做 ROC Curve。AUC 其实就是这个曲线下的面积了。AUC 有一个很重要的统计特性：AUC 值等于分类器对随机选择的正样本的预测值高于对随机选择的负样本的预测值的概率。AUC 的详细介绍和分析可以看这篇论文：An introduction to ROC analysis.
在说 ROC 之前，先说一下二值分类器的一些常用术语。如下所示，左边是模型预测的结果，上面是实际的分类情况。
    True Class 1 0     Prediction 1 TP FP    0 FN TN    来看下图，ROC 的横轴是 FPR (False Positive Rate)，纵轴是 TPR (True Positive Rate)，它们的定义分别如下：
$$ FPR = \frac{FP}{FP + TN}$$ $$ TPR = \frac{TP}{TP+FN} $$ 不妨来看图中的 4 个顶点：</description>
    </item>
    
    <item>
      <title>Python 相关的数据挖掘利器</title>
      <link>http://m.sjchn.com/p/2016/python-data-mining/</link>
      <pubDate>Thu, 25 Feb 2016 00:00:00 +0000</pubDate>
      
      <guid>http://m.sjchn.com/p/2016/python-data-mining/</guid>
      <description>首先需要反省一下自己，对新事物的好奇心不够强烈，很多时候都是浅尝辄止，没有深入去了解，以致错失了很多机会，也让我不能看透很多事情，很多方面不能形成自己的知识积累。是一个极大的缺点，需要改正。
其实几年前就听说过 IPython，当时以为只是 Python 的另外一个 REPL 终端而已，没有深入去了解。
平时工作中的数据量都很大，一般都是存储在 HDFS 上，加载到单机上比较费劲，都是用 Spark (之前用 Pig) 去分析，偶尔也会写一些一次性的 Python 脚本，有作图需求时都是把数据分析出来导入 Excel。大部分情况下这样其实还可以。
只是最近试着参加了 kaggle 上的 Airbnb recruiting 比赛，见识到了 Python 系列工具在数据挖掘中的强大。而且也加深了对 IPython 的认识，尤其是 IPython Notebook (或者叫 Jupyter Notebook) 的强大和便捷。
NumPy, matplotlib, xgboost 和 scikt-learn 以前也接触过，了解一些。但 Pandas, Seaborn 等还真没用过。看着别人把这些工具耍得贼熟，感觉自己像原始人一样站在一边…… 虽说用 Spark 来搞也没啥问题，但是毕竟有点麻烦。不学习一下真是说不过去，就会被别人远远地抛在后面。
如果要用 Python 做数据分析相关工作，可以直接装 Anaconda。它打包了很多有用的库，另外可能需要单独装下 xgboost 和 seaborn（或者还有其它的包，试一下就知道）。完成这些后，基于 Python 的数据分析平台环境就搭建好了。具体 Pandas, xgboost, seaborn 等的应用可以查看各自的文档。话说 Pandas 里的 DataFrame 真是很强大，Spark 中新的 ml 库 (替换旧的 mllib) 也大量使用了 DataFrame。
还有一个强大的工具，Jupyter Notebook.</description>
    </item>
    
    <item>
      <title>Windows 下搭建 Spark 源代码阅读环境</title>
      <link>http://m.sjchn.com/p/2015/win-spark-src-env/</link>
      <pubDate>Mon, 14 Sep 2015 00:00:00 +0000</pubDate>
      
      <guid>http://m.sjchn.com/p/2015/win-spark-src-env/</guid>
      <description>用 Spark 做数据分析也有半年多了，觉得至少应该大致过一下源代码。在 Windows 下搭建环境还是有点不太方便的，把步骤记录一下。
当然要用 Intellij IDEA (下文简称 Idea )，安装 Scala 插件。另外，也可以把 sbt 插件装上。
首先下载 Spark 1.5.0 源代码。
然后在 Idea 里， File -&amp;gt; New -&amp;gt; Project From Existing Source 选择 Spark 解压后的目录， Import project from external model -&amp;gt; Maven，然后一路 Next，等待处理完毕。
视网络情况而定，不太好的话可能需要半个小时。然后就可以编译 Spark 了：Build -&amp;gt; Rebuild Project
当然不一定成功，如果报的错误是 Error:(44, 66) not found: type SparkFlumeProtocol val 的话，做如下处理：
View -&amp;gt; Tool Window -&amp;gt; Maven Projects 选择 Spark Project External Flume Sink， 右键 -&amp;gt; Generate Sources and Update Folders  之后一般就能编译成功了，会有很多 Warning，都可以忽略。</description>
    </item>
    
  </channel>
</rss>